{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Part_final.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "code",
        "id": "7vL1jyNnEPzJ",
        "outputId": "de40a423-3fd0-436a-b1fb-fe83071ec61c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        }
      },
      "cell_type": "code",
      "source": [
        "# Install a Drive FUSE wrapper.\n",
        "# https://github.com/astrada/google-drive-ocamlfuse\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "EWIf6ft6GNHO",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Generate auth tokens for Colab\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "r-mKXNLYGNQW",
        "outputId": "3c0aec91-22f4-4147-c5f5-d41304e8aca9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "cell_type": "code",
      "source": [
        "# Generate creds for the Drive FUSE library.\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "sKPo0KoXGR1V",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a directory and mount Google Drive using that directory.\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7yvWmFbtAf9k",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## This is not working for IU\n",
        "## Load the Drive helper and mount\n",
        "# from google.colab import drive\n",
        "\n",
        "## This will prompt for authorization.\n",
        "# drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "P58I9X7VCZmE",
        "outputId": "227aa0ad-2c60-4870-a683-72ff59b1d7c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1020
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision librosa seaborn==0.9"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/60/66415660aa46b23b5e1b72bc762e816736ce8d7260213e22365af51e8f9c/torch-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (591.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 591.8MB 22kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x61462000 @  0x7f1c0cb362a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "\u001b[?25hCollecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 21.2MB/s \n",
            "\u001b[?25hCollecting librosa\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/b4/5b411f19de48f8fc1a0ff615555aa9124952e4156e94d4803377e50cfa4c/librosa-0.6.2.tar.gz (1.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.6MB 7.0MB/s \n",
            "\u001b[?25hCollecting seaborn==0.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/76/220ba4420459d9c4c9c9587c6ce607bf56c25b3d3d2de62056efe482dadc/seaborn-0.9.0-py3-none-any.whl (208kB)\n",
            "\u001b[K    100% |████████████████████████████████| 215kB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 5.9MB/s \n",
            "\u001b[?25hCollecting audioread>=2.0.0 (from librosa)\n",
            "  Downloading https://files.pythonhosted.org/packages/f0/41/8cd160c6b2046b997d571a744a7f398f39e954a62dd747b2aae1ad7f07d4/audioread-2.1.6.tar.gz\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.19.2)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.13.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (4.3.0)\n",
            "Collecting resampy>=0.2.0 (from librosa)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/b6/66a06d85474190b50aee1a6c09cdc95bb405ac47338b27e9b21409da1760/resampy-0.2.1.tar.gz (322kB)\n",
            "\u001b[K    100% |████████████████████████████████| 327kB 26.6MB/s \n",
            "\u001b[?25hCollecting numba>=0.38.0 (from librosa)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/55/938f0023a4f37fe24460d46846670aba8170a6b736f1693293e710d4a6d0/numba-0.41.0-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.2MB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=1.4.3 in /usr/local/lib/python3.6/dist-packages (from seaborn==0.9) (2.1.2)\n",
            "Requirement already satisfied: pandas>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from seaborn==0.9) (0.22.0)\n",
            "Collecting llvmlite>=0.26.0dev0 (from numba>=0.38.0->librosa)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/60/d22966c97a47687ac1cc57c2e756380897c264f1ce40780105d7dbcd9564/llvmlite-0.26.0-cp36-cp36m-manylinux1_x86_64.whl (16.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 16.1MB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn==0.9) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn==0.9) (2.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn==0.9) (2.5.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn==0.9) (2018.7)\n",
            "Building wheels for collected packages: librosa, audioread, resampy\n",
            "  Running setup.py bdist_wheel for librosa ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/18/b8/10/f0f8f6ac60668a5cd75596cf14c25bb6b3ea1ecd815f058b7e\n",
            "  Running setup.py bdist_wheel for audioread ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/53/02/90/7b5c4081b7470c550ab605f600bad237dde12a6b8999b11f50\n",
            "  Running setup.py bdist_wheel for resampy ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ff/4f/ed/2e6c676c23efe5394bb40ade50662e90eb46e29b48324c5f9b\n",
            "Successfully built librosa audioread resampy\n",
            "Installing collected packages: torch, pillow, torchvision, audioread, llvmlite, numba, resampy, librosa, seaborn\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "  Found existing installation: seaborn 0.7.1\n",
            "    Uninstalling seaborn-0.7.1:\n",
            "      Successfully uninstalled seaborn-0.7.1\n",
            "Successfully installed audioread-2.1.6 librosa-0.6.2 llvmlite-0.26.0 numba-0.41.0 pillow-5.3.0 resampy-0.2.1 seaborn-0.9.0 torch-1.0.0 torchvision-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "nGtVStKtv6LT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "500f07cd-2f74-4ae5-8fce-1df0ba4565bc"
      },
      "cell_type": "code",
      "source": [
        "SERVER = True\n",
        "\n",
        "# Files and Directories\n",
        "ROOT_DIR = \"drive/SIV-Project/\"\n",
        "TRAIN_PATH = 'wav_train_subset'\n",
        "STFT_FOLDER = 'stft'\n",
        "CHECKPOINTS_FOLDER = \"checkpoints\"\n",
        "PAIRS_FILE = 'pairs.csv'\n",
        "VGG_VOX_WEIGHT_FILE = \"vggvox_ident_net.mat\"\n",
        "\n",
        "# Data_Part\n",
        "TOTAL_USERS = 100\n",
        "CLIPS_PER_USER = 10\n",
        "MIN_CLIP_DURATION = 3.\n",
        "\n",
        "# ML_Part\n",
        "TRAINING_USERS = 80\n",
        "SIMILAR_PAIRS = 20\n",
        "DISSIMILAR_PAIRS = SIMILAR_PAIRS\n",
        "\n",
        "\n",
        "assert SIMILAR_PAIRS <= CLIPS_PER_USER * (CLIPS_PER_USER - 1)\n",
        "# print(\"len of pairs.csv should be\", (SIMILAR_PAIRS + DISSIMILAR_PAIRS) * TOTAL_USERS)\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import time\n",
        "import itertools\n",
        "from collections import Counter\n",
        "from IPython.core.display import HTML\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.io import loadmat\n",
        "import scipy\n",
        "import sklearn\n",
        "import librosa\n",
        "import librosa.display\n",
        "import wave\n",
        "import contextlib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "\n",
        "if SERVER:\n",
        "    sys.path.insert(0, ROOT_DIR)\n",
        "\n",
        "def get_rel_path(path, server=SERVER, root_dir=ROOT_DIR):\n",
        "    if server:\n",
        "        return os.path.join(root_dir, path)\n",
        "    else:\n",
        "        return path\n",
        "\n",
        "\n",
        "def wavPlayer(filepath):\n",
        "    src = \"\"\"\n",
        "    <head>\n",
        "    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n",
        "    <title>Simple Test</title>\n",
        "    </head>\n",
        "\n",
        "    <body>\n",
        "    <audio controls=\"controls\" style=\"width:600px\" >\n",
        "      <source src=\"%s\" type=\"audio/wav\" />\n",
        "      Your browser does not support the audio element.\n",
        "    </audio>\n",
        "    </body>\n",
        "    \"\"\"%(filepath)\n",
        "    display(HTML(src))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a03b0c16d68b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwave\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'librosa'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "i6aZ0NykBT70",
        "outputId": "134978d1-3f04-456d-d08e-d382ba6e0629",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1swM4U97IAeK",
        "outputId": "d745b67b-6e68-43be-efa1-9feeea9c7551",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "weights = {}\n",
        "\n",
        "# loading pretrained vog_vgg learned weights\n",
        "vox_weights = loadmat(get_rel_path(VGG_VOX_WEIGHT_FILE), \n",
        "                      struct_as_record=False, squeeze_me=True)\n",
        "\n",
        "for l in vox_weights['net'].layers[:-1]:\n",
        "    if len(l.weights) > 0:\n",
        "        weights[l.name] = l.weights\n",
        "        print(l.name, [i.shape for i in l.weights])\n",
        "        \n",
        "for i in weights:\n",
        "    weights[i][0] = weights[i][0].T \n",
        "\n",
        "weights['conv1'][0] = np.expand_dims(weights['conv1'][0], axis=1)\n",
        "weights['fc6'][0] = np.expand_dims(weights['fc6'][0], axis=3)\n",
        "weights['fc7'][0] = np.expand_dims(weights['fc7'][0], axis=-1)\n",
        "weights['fc7'][0] = np.expand_dims(weights['fc7'][0], axis=-1)\n",
        "\n",
        "print(weights.keys())   \n",
        "for key in weights:\n",
        "    print(key, [i.shape for i in weights[key]])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv1 [(7, 7, 96), (96,)]\n",
            "conv2 [(5, 5, 96, 256), (256,)]\n",
            "conv3 [(3, 3, 256, 384), (384,)]\n",
            "conv4 [(3, 3, 384, 256), (256,)]\n",
            "conv5 [(3, 3, 256, 256), (256,)]\n",
            "fc6 [(9, 256, 4096), (4096,)]\n",
            "fc7 [(4096, 1024), (1024,)]\n",
            "fc8 [(1024, 1300), (1300,)]\n",
            "dict_keys(['conv1', 'conv2', 'conv3', 'conv4', 'conv5', 'fc6', 'fc7', 'fc8'])\n",
            "conv1 [(96, 1, 7, 7), (96,)]\n",
            "conv2 [(256, 96, 5, 5), (256,)]\n",
            "conv3 [(384, 256, 3, 3), (384,)]\n",
            "conv4 [(256, 384, 3, 3), (256,)]\n",
            "conv5 [(256, 256, 3, 3), (256,)]\n",
            "fc6 [(4096, 256, 9, 1), (4096,)]\n",
            "fc7 [(1024, 4096, 1, 1), (1024,)]\n",
            "fc8 [(1300, 1024), (1300,)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Z0jMmErb3Nv-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "conv_kernel1, n_f1, s1, p1 = 7, 96, 2, 1\n",
        "pool_kernel1, pool_s1 = 3, 2\n",
        "\n",
        "conv_kernel2, n_f2, s2, p2 = 5, 256, 2, 1\n",
        "pool_kernel2, pool_s2 = 3, 2\n",
        "\n",
        "conv_kernel3, n_f3, s3, p3 = 3, 384, 1, 1\n",
        "\n",
        "conv_kernel4, n_f4, s4, p4 = 3, 256, 1, 1\n",
        "\n",
        "conv_kernel5, n_f5, s5, p5 = 3, 256, 1, 1\n",
        "pool_kernel5_x, pool_kernel5_y, pool_s5_x, pool_s5_y = 5, 3, 3, 2\n",
        "\n",
        "conv_kernel6_x, conv_kernel6_y, n_f6, s6 = 9, 1, 4096, 1\n",
        "\n",
        "conv_kernel7, n_f7, s7 = 1, 1024, 1\n",
        "\n",
        "conv_kernel8, n_f8, s8 = 1, 1024, 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "S88jByoxIA0w",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VggVox(nn.Module):\n",
        "    '''\n",
        "    Class for CNN architecture (VGGvox)\n",
        "    '''\n",
        "    def __init__(self, use_weights=True):\n",
        "        super(VggVox, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(in_channels=1, \n",
        "                                             out_channels=n_f1, \n",
        "                                             kernel_size=conv_kernel1,\n",
        "                                             stride=s1,\n",
        "                                             padding=p1),\n",
        "                                    nn.BatchNorm2d(num_features=n_f1),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.MaxPool2d(kernel_size=pool_kernel1,\n",
        "                                                 stride=pool_s1))           \n",
        "            \n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(in_channels=n_f1,\n",
        "                                         out_channels=n_f2,\n",
        "                                         kernel_size=conv_kernel2,\n",
        "                                         stride=s2,\n",
        "                                         padding=p2),\n",
        "                                nn.BatchNorm2d(num_features=n_f2),\n",
        "                                nn.ReLU(),\n",
        "                                nn.MaxPool2d(kernel_size=pool_kernel2,\n",
        "                                             stride=pool_s2))\n",
        "\n",
        "\n",
        "        self.conv3 = nn.Sequential(nn.Conv2d(in_channels=n_f2,\n",
        "                                             out_channels=n_f3,\n",
        "                                             kernel_size=conv_kernel3,\n",
        "                                             stride=s3,\n",
        "                                             padding=p3),\n",
        "                                    nn.BatchNorm2d(num_features=n_f3),\n",
        "                                    nn.ReLU())\n",
        "\n",
        "        self.conv4 = nn.Sequential(nn.Conv2d(in_channels=n_f3,\n",
        "                                             out_channels=n_f4,\n",
        "                                             kernel_size=conv_kernel4,\n",
        "                                             stride=s4,\n",
        "                                             padding=p4),\n",
        "                                   nn.BatchNorm2d(num_features=n_f4),\n",
        "                                   nn.ReLU())\n",
        "\n",
        "        self.conv5 = nn.Sequential(nn.Conv2d(in_channels=n_f4,\n",
        "                                             out_channels=n_f5,\n",
        "                                             kernel_size=conv_kernel5,\n",
        "                                             stride=s5,\n",
        "                                             padding=p5),\n",
        "                                   nn.BatchNorm2d(num_features=n_f5),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.MaxPool2d(kernel_size=(pool_kernel5_x, pool_kernel5_y),\n",
        "                                                stride=(pool_s5_x, pool_s5_y)))\n",
        "\n",
        "        self.fc6 = nn.Sequential(nn.Conv2d(in_channels=n_f5,\n",
        "                                             out_channels=n_f6,\n",
        "                                             kernel_size=(conv_kernel6_x, conv_kernel6_y),\n",
        "                                             stride=s6),\n",
        "                                 nn.BatchNorm2d(num_features=n_f6), \n",
        "                                 nn.ReLU())\n",
        "\n",
        "        self.global_pool = nn.AvgPool2d\n",
        "\n",
        "        self.fc7 = nn.Sequential(nn.Conv2d(in_channels=n_f6,\n",
        "                                           out_channels=n_f7,\n",
        "                                           kernel_size=conv_kernel7,\n",
        "                                           stride=s7),\n",
        "                                 nn.ReLU())\n",
        "\n",
        "        self.fc8 = nn.Sequential(nn.Conv2d(in_channels=n_f7,\n",
        "                                           out_channels=n_f8,\n",
        "                                           kernel_size=conv_kernel8,\n",
        "                                           stride=s8))\n",
        "   \n",
        "        if use_weights:\n",
        "            self.conv1[0].weight = torch.nn.Parameter(torch.from_numpy(weights['conv1'][0]))\n",
        "            self.conv1[1].weight = torch.nn.Parameter(torch.from_numpy(weights['conv1'][1]))\n",
        "            \n",
        "            self.conv2[0].weight = torch.nn.Parameter(torch.from_numpy(weights['conv2'][0]))\n",
        "            self.conv2[1].weight = torch.nn.Parameter(torch.from_numpy(weights['conv2'][1]))\n",
        "            \n",
        "            self.conv3[0].weight = torch.nn.Parameter(torch.from_numpy(weights['conv3'][0]))\n",
        "            self.conv3[1].weight = torch.nn.Parameter(torch.from_numpy(weights['conv3'][1]))\n",
        "            \n",
        "            self.conv4[0].weight = torch.nn.Parameter(torch.from_numpy(weights['conv4'][0]))\n",
        "            self.conv4[1].weight = torch.nn.Parameter(torch.from_numpy(weights['conv4'][1]))\n",
        "            \n",
        "            self.conv5[0].weight = torch.nn.Parameter(torch.from_numpy(weights['conv5'][0]))\n",
        "            self.conv5[1].weight = torch.nn.Parameter(torch.from_numpy(weights['conv5'][1]))            \n",
        "            \n",
        "            self.fc6[0].weight = torch.nn.Parameter(torch.from_numpy(weights['fc6'][0]))\n",
        "            self.fc6[1].weight = torch.nn.Parameter(torch.from_numpy(weights['fc6'][1]))\n",
        "            \n",
        "            self.fc7[0].weight = torch.nn.Parameter(torch.from_numpy(weights['fc7'][0]))\n",
        "   \n",
        "    def forward_single(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.conv5(x)\n",
        "        x = self.fc6(x)\n",
        "        x = self.global_pool(kernel_size=x.size()[2:])(x)\n",
        "        x = self.fc7(x)\n",
        "        out = self.fc8(x)      \n",
        "        out = out.view(-1, out.shape[1])\n",
        "        return out\n",
        "  \n",
        "    def forward(self, input1, input2):\n",
        "        output1 = self.forward_single(input1)\n",
        "        output2 = self.forward_single(input2)\n",
        "        return output1, output2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4GNo5xzFILc6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ContrastiveLoss(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Contrastive loss function.\n",
        "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, margin=2.0):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, output1, output2, label):\n",
        "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
        "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
        "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
        "        return loss_contrastive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "91Xm-2JiILyF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VoxCelebDataset(Dataset):\n",
        "    def __init__(self, pairs_fname=PAIRS_FILE, n_users=TRAINING_USERS, clips_per_user=CLIPS_PER_USER):\n",
        "        pairs_file = pd.read_csv(get_rel_path(pairs_fname))\n",
        "        self.all_user_ids = sorted(pairs_file.user1.unique())\n",
        "        self.training_users = self.all_user_ids[: n_users]\n",
        "        \n",
        "        user1_subset = pairs_file[pairs_file.user1.isin(self.training_users)]\n",
        "        user2_subset = user1_subset[user1_subset.user2.isin(self.training_users)]\n",
        "        \n",
        "        def balance_data(df):\n",
        "            pairs_df = []\n",
        "            for user in df.user1.unique():\n",
        "                user_df = df[df.user1 == user]\n",
        "                similar = user_df[user_df['label'] == 0].sample(n=SIMILAR_PAIRS)\n",
        "                dissimilar = user_df[user_df['label'] == 1].sample(n=DISSIMILAR_PAIRS)\n",
        "                pairs_df.append(pd.concat([similar, dissimilar]))\n",
        "\n",
        "            pairs_df = pd.concat(pairs_df)\n",
        "            return pairs_df\n",
        "                \n",
        "        pairs_df = balance_data(user2_subset)\n",
        "        \n",
        "        assert len(pairs_df[pairs_df.user1.isin(self.training_users)]) == len(pairs_df)\n",
        "        assert len(pairs_df[pairs_df.user2.isin(self.training_users)]) == len(pairs_df)\n",
        "        \n",
        "        self.spec = pairs_df[['path1', 'path2', 'label']].values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.spec)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        spec1_path = get_rel_path(self.spec[idx][0])\n",
        "        spec2_path = get_rel_path(self.spec[idx][1])\n",
        "        label = int(self.spec[idx][2])\n",
        "        \n",
        "        spec1 = np.load(spec1_path)\n",
        "        spec2 = np.load(spec2_path)\n",
        "        \n",
        "        spec1 = np.expand_dims(spec1, axis=0)\n",
        "        spec2 = np.expand_dims(spec2, axis=0)\n",
        "        \n",
        "        assert spec1.ndim == 3, spec2.ndim == 3\n",
        "        \n",
        "        sample = {'spec1': spec1, 'spec2': spec2, 'label': label}\n",
        "\n",
        "        return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "z100FbkIYslN",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def save_checkpoint(state, loss):\n",
        "    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n",
        "    fname = \"checkpoint_\" + time.strftime(\"%Y%m%d-%H%M%S\") + \"_\" + str(loss.item()) + \".pth.tar\"\n",
        "    torch.save(state, get_rel_path(os.path.join(CHECKPOINTS_FOLDER, fname)))  # save checkpoint\n",
        "    print(\"$$$ Saved a new checkpoint\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pfoWVXiKIWEm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_saved_model(fname, test=True):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    new_model_dict = VggVox()\n",
        "    checkpoint_path = get_rel_path(os.path.join(CHECKPOINTS_FOLDER, fname))\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "    new_model_dict.load_state_dict(checkpoint['state_dict'])\n",
        "    if test:\n",
        "        model = new_model_dict.eval()\n",
        "    model = model.to(device)\n",
        "\n",
        "    new_optimizer = optim.Adam(params=model.parameters())\n",
        "    new_optimizer.load_state_dict(checkpoint['optim_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "\n",
        "    return model, new_model_dict, new_optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Bh80WU4lJAxE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 5e-3\n",
        "N_EPOCHS = 10\n",
        "BATCH_SIZE = 8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lIKadKjFDTgx",
        "outputId": "6f187bdb-e63d-4e9b-d20f-b5d67cea8a07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "voxceleb_dataset = VoxCelebDataset(PAIRS_FILE)\n",
        "train_dataloader = DataLoader(voxceleb_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
        "                              num_workers=4)\n",
        "n_batches = int(len(voxceleb_dataset) / BATCH_SIZE)\n",
        "\n",
        "print(\"training unique users\", len(voxceleb_dataset.training_users))\n",
        "print(\"training samples\", len(voxceleb_dataset))\n",
        "print(\"batches\", int(len(voxceleb_dataset) / BATCH_SIZE))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training unique users 80\n",
            "training samples 3200\n",
            "batches 400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "coB4oXXDIOqp",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = VggVox(use_weights=True)\n",
        "model = model.to(device)\n",
        "# print(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZnjSEjKOwjGV",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "criterion = ContrastiveLoss()\n",
        "criterion = criterion"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "sDMrogyLwjGY",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "OUQYaOUjwjGb",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss_list = []\n",
        "best_loss = torch.autograd.Variable(torch.tensor(np.inf)).float()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FbWVd_VNIVYr",
        "outputId": "9b55d499-8dcf-4f54-80de-f208b94acdd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3519
        }
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(1, N_EPOCHS+1):\n",
        "    running_loss = torch.zeros(1)\n",
        "    \n",
        "    for i_batch, data in enumerate(train_dataloader, 1):\n",
        "        mfcc1, mfcc2, label = data['spec1'], data['spec2'], data['label']\n",
        "        mfcc1 = Variable(mfcc1.float(), requires_grad=True).to(device)\n",
        "        mfcc2 = Variable(mfcc2.float(), requires_grad=True).to(device)\n",
        "        label = Variable(label.float(), requires_grad=True).to(device)\n",
        "                \n",
        "        output1, output2 = model(mfcc1.float(), mfcc2.float())\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = criterion(output1, output2, label.float())\n",
        "        \n",
        "#         assert mfcc1.dim() == mfcc2.dim() == 4        \n",
        "#         assert output1.dim() == output2.dim() == 2\n",
        "#         assert loss.requires_grad and output1.requires_grad and output2.requires_grad\n",
        "#         assert loss.grad_fn is not None and output1.grad_fn is not None and output2.grad_fn is not None \n",
        "        \n",
        "#         print(\"loss\", loss, loss.requires_grad, loss.grad_fn)\n",
        "#         print(\"output1\", output1.shape, output1.requires_grad, output1.grad_fn, output1.device)\n",
        "#         print(\"output2\", output2.shape, output2.requires_grad, output2.grad_fn, output2.device)\n",
        "\n",
        "        loss.backward()\n",
        "            \n",
        "#         assert mfcc1.requires_grad and mfcc2.requires_grad                \n",
        "#         for name, param in model.named_parameters():\n",
        "#             assert param.requires_grad and param.grad is not None, (name, param.requires_grad, param.grad)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "        running_loss += loss.item()\n",
        "        if i_batch % int(n_batches/20) == 0:\n",
        "            print(\"Epoch {}/{}  Batch {}/{} \\nCurrent Batch Loss {}\\n\".format(epoch, N_EPOCHS, i_batch, n_batches, loss.item()))\n",
        "        \n",
        "    epoch_loss = running_loss / len(voxceleb_dataset)\n",
        "    print(\"==> Epoch {}/{} Epoch Loss {}\".format(epoch, N_EPOCHS, epoch_loss.item()))\n",
        "\n",
        "    is_best = epoch_loss < best_loss\n",
        "    if is_best:\n",
        "        best_loss = epoch_loss\n",
        "        \n",
        "        save_checkpoint({'epoch': epoch,\n",
        "                         'state_dict': model.state_dict(),\n",
        "                         'optim_dict': optimizer.state_dict()},\n",
        "                        loss=epoch_loss)\n",
        "    else:\n",
        "        print(\"### Epoch Loss did not improve\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10  Batch 20/400 \n",
            "Current Batch Loss 2.8185806274414062\n",
            "\n",
            "Epoch 1/10  Batch 40/400 \n",
            "Current Batch Loss 1.3594204187393188\n",
            "\n",
            "Epoch 1/10  Batch 60/400 \n",
            "Current Batch Loss 1.3129148483276367\n",
            "\n",
            "Epoch 1/10  Batch 80/400 \n",
            "Current Batch Loss 0.880825936794281\n",
            "\n",
            "Epoch 1/10  Batch 100/400 \n",
            "Current Batch Loss 0.9434598684310913\n",
            "\n",
            "Epoch 1/10  Batch 120/400 \n",
            "Current Batch Loss 1.0217459201812744\n",
            "\n",
            "Epoch 1/10  Batch 140/400 \n",
            "Current Batch Loss 1.0650825500488281\n",
            "\n",
            "Epoch 1/10  Batch 160/400 \n",
            "Current Batch Loss 1.5629602670669556\n",
            "\n",
            "Epoch 1/10  Batch 180/400 \n",
            "Current Batch Loss 1.12093186378479\n",
            "\n",
            "Epoch 1/10  Batch 200/400 \n",
            "Current Batch Loss 1.4232218265533447\n",
            "\n",
            "Epoch 1/10  Batch 220/400 \n",
            "Current Batch Loss 0.6746394038200378\n",
            "\n",
            "Epoch 1/10  Batch 240/400 \n",
            "Current Batch Loss 0.7833440899848938\n",
            "\n",
            "Epoch 1/10  Batch 260/400 \n",
            "Current Batch Loss 1.1394896507263184\n",
            "\n",
            "Epoch 1/10  Batch 280/400 \n",
            "Current Batch Loss 1.030495524406433\n",
            "\n",
            "Epoch 1/10  Batch 300/400 \n",
            "Current Batch Loss 0.9562946557998657\n",
            "\n",
            "Epoch 1/10  Batch 320/400 \n",
            "Current Batch Loss 1.3603956699371338\n",
            "\n",
            "Epoch 1/10  Batch 340/400 \n",
            "Current Batch Loss 0.919370710849762\n",
            "\n",
            "Epoch 1/10  Batch 360/400 \n",
            "Current Batch Loss 0.8345241546630859\n",
            "\n",
            "Epoch 1/10  Batch 380/400 \n",
            "Current Batch Loss 1.3581997156143188\n",
            "\n",
            "Epoch 1/10  Batch 400/400 \n",
            "Current Batch Loss 1.6061058044433594\n",
            "\n",
            "=> Epoch 1/10 Epoch Loss 0.558148980140686\n",
            "$$ Saved a new checkpoint\n",
            "\n",
            "Epoch 2/10  Batch 20/400 \n",
            "Current Batch Loss 1.2777153253555298\n",
            "\n",
            "Epoch 2/10  Batch 40/400 \n",
            "Current Batch Loss 1.1331043243408203\n",
            "\n",
            "Epoch 2/10  Batch 60/400 \n",
            "Current Batch Loss 1.119394063949585\n",
            "\n",
            "Epoch 2/10  Batch 80/400 \n",
            "Current Batch Loss 1.033261775970459\n",
            "\n",
            "Epoch 2/10  Batch 100/400 \n",
            "Current Batch Loss 1.4137541055679321\n",
            "\n",
            "Epoch 2/10  Batch 120/400 \n",
            "Current Batch Loss 0.9975684881210327\n",
            "\n",
            "Epoch 2/10  Batch 140/400 \n",
            "Current Batch Loss 1.1796534061431885\n",
            "\n",
            "Epoch 2/10  Batch 160/400 \n",
            "Current Batch Loss 0.9646154046058655\n",
            "\n",
            "Epoch 2/10  Batch 180/400 \n",
            "Current Batch Loss 1.318661093711853\n",
            "\n",
            "Epoch 2/10  Batch 200/400 \n",
            "Current Batch Loss 0.8038895726203918\n",
            "\n",
            "Epoch 2/10  Batch 220/400 \n",
            "Current Batch Loss 1.2592427730560303\n",
            "\n",
            "Epoch 2/10  Batch 240/400 \n",
            "Current Batch Loss 0.49244067072868347\n",
            "\n",
            "Epoch 2/10  Batch 260/400 \n",
            "Current Batch Loss 0.9802789688110352\n",
            "\n",
            "Epoch 2/10  Batch 280/400 \n",
            "Current Batch Loss 0.7267194986343384\n",
            "\n",
            "Epoch 2/10  Batch 300/400 \n",
            "Current Batch Loss 1.6284245252609253\n",
            "\n",
            "Epoch 2/10  Batch 320/400 \n",
            "Current Batch Loss 0.7577006220817566\n",
            "\n",
            "Epoch 2/10  Batch 340/400 \n",
            "Current Batch Loss 1.1452836990356445\n",
            "\n",
            "Epoch 2/10  Batch 360/400 \n",
            "Current Batch Loss 1.8772075176239014\n",
            "\n",
            "Epoch 2/10  Batch 380/400 \n",
            "Current Batch Loss 0.9663788080215454\n",
            "\n",
            "Epoch 2/10  Batch 400/400 \n",
            "Current Batch Loss 1.1596739292144775\n",
            "\n",
            "=> Epoch 2/10 Epoch Loss 0.1343759149312973\n",
            "$$ Saved a new checkpoint\n",
            "\n",
            "Epoch 3/10  Batch 20/400 \n",
            "Current Batch Loss 1.057780385017395\n",
            "\n",
            "Epoch 3/10  Batch 40/400 \n",
            "Current Batch Loss 1.083954095840454\n",
            "\n",
            "Epoch 3/10  Batch 60/400 \n",
            "Current Batch Loss 0.7919422388076782\n",
            "\n",
            "Epoch 3/10  Batch 80/400 \n",
            "Current Batch Loss 0.8884207010269165\n",
            "\n",
            "Epoch 3/10  Batch 100/400 \n",
            "Current Batch Loss 1.6517945528030396\n",
            "\n",
            "Epoch 3/10  Batch 120/400 \n",
            "Current Batch Loss 1.5672941207885742\n",
            "\n",
            "Epoch 3/10  Batch 140/400 \n",
            "Current Batch Loss 0.943871796131134\n",
            "\n",
            "Epoch 3/10  Batch 160/400 \n",
            "Current Batch Loss 1.3142178058624268\n",
            "\n",
            "Epoch 3/10  Batch 180/400 \n",
            "Current Batch Loss 0.6127852201461792\n",
            "\n",
            "Epoch 3/10  Batch 200/400 \n",
            "Current Batch Loss 0.8938034772872925\n",
            "\n",
            "Epoch 3/10  Batch 220/400 \n",
            "Current Batch Loss 0.45550844073295593\n",
            "\n",
            "Epoch 3/10  Batch 240/400 \n",
            "Current Batch Loss 1.1254819631576538\n",
            "\n",
            "Epoch 3/10  Batch 260/400 \n",
            "Current Batch Loss 1.467857837677002\n",
            "\n",
            "Epoch 3/10  Batch 280/400 \n",
            "Current Batch Loss 1.5526149272918701\n",
            "\n",
            "Epoch 3/10  Batch 300/400 \n",
            "Current Batch Loss 1.2845025062561035\n",
            "\n",
            "Epoch 3/10  Batch 320/400 \n",
            "Current Batch Loss 0.718665361404419\n",
            "\n",
            "Epoch 3/10  Batch 340/400 \n",
            "Current Batch Loss 0.6838468313217163\n",
            "\n",
            "Epoch 3/10  Batch 360/400 \n",
            "Current Batch Loss 1.7117326259613037\n",
            "\n",
            "Epoch 3/10  Batch 380/400 \n",
            "Current Batch Loss 1.952256202697754\n",
            "\n",
            "Epoch 3/10  Batch 400/400 \n",
            "Current Batch Loss 2.499920129776001\n",
            "\n",
            "=> Epoch 3/10 Epoch Loss 0.14908283948898315\n",
            "## Epoch Loss did not improve\n",
            "Epoch 4/10  Batch 20/400 \n",
            "Current Batch Loss 0.4999840259552002\n",
            "\n",
            "Epoch 4/10  Batch 40/400 \n",
            "Current Batch Loss 1.9999361038208008\n",
            "\n",
            "Epoch 4/10  Batch 60/400 \n",
            "Current Batch Loss 1.4999520778656006\n",
            "\n",
            "Epoch 4/10  Batch 80/400 \n",
            "Current Batch Loss 1.9999361038208008\n",
            "\n",
            "Epoch 4/10  Batch 100/400 \n",
            "Current Batch Loss 2.999904155731201\n",
            "\n",
            "Epoch 4/10  Batch 120/400 \n",
            "Current Batch Loss 1.9999361038208008\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o_8nmcBcIPJw",
        "colab_type": "code",
        "outputId": "f94a84f1-710b-4d93-af4b-0f887668f53a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        }
      },
      "cell_type": "code",
      "source": [
        "plt.plot(loss_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f71e205e978>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 717, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 713, in _shutdown_workers\n",
            "    w.join()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 124, in join\n",
            "    res = self._popen.wait(timeout)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/popen_fork.py\", line 50, in wait\n",
            "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/popen_fork.py\", line 28, in poll\n",
            "    pid, sts = os.waitpid(self.pid, flag)\n",
            "KeyboardInterrupt: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f71e01ae358>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAFKCAYAAADScRzUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIlLAFNQiIiIF7P8D4KUGnsgFq+8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f71e1fb7b38>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "OAHMSoKDNKrd",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if is_best:\n",
        "    save_checkpoint({'epoch': epoch,\n",
        "                     'state_dict': model.state_dict(),\n",
        "                     'optim_dict': optimizer.state_dict()}, \n",
        "                    is_best=is_best, loss=epoch_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "BAXVwq4oTfmx",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "if cuda:\n",
        "    checkpoint = torch.load(resume_weights)\n",
        "else:\n",
        "    # Load GPU model on CPU\n",
        "    checkpoint = torch.load(resume_weights,\n",
        "                            map_location=lambda storage,\n",
        "                            loc: storage)\n",
        "start_epoch = checkpoint['epoch']\n",
        "best_accuracy = checkpoint['best_accuracy']\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "print(\"=> loaded checkpoint '{}' (trained for {} epochs)\".format(resume_weights, checkpoint['epoch']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Lcl2k54IPJ2",
        "colab_type": "code",
        "outputId": "9c7acf32-7f6f-4cae-f5d1-f7ea02599872",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "cell_type": "code",
      "source": [
        "plt.plot(loss_list[-400:])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0e46195a7be6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "CtTN1pY_PhJv",
        "colab_type": "code",
        "outputId": "b06bbc83-448c-4241-867c-ebe4082bb72a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.2253962755203247"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "metadata": {
        "id": "I9BxV-3KPwhu",
        "colab_type": "code",
        "outputId": "7652a4c1-929d-46dd-a32c-3070ff8b7a33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "sns.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.7.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "metadata": {
        "id": "qKzVCuTsQhO0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "load_saved_model()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}